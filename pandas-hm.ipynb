{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-18T22:36:15.302848Z","iopub.execute_input":"2021-11-18T22:36:15.303706Z","iopub.status.idle":"2021-11-18T22:36:15.314502Z","shell.execute_reply.started":"2021-11-18T22:36:15.303655Z","shell.execute_reply":"2021-11-18T22:36:15.313735Z"},"trusted":true},"execution_count":165,"outputs":[]},{"cell_type":"markdown","source":"# **1.Read the dataset as a dataframe. Create a copy of your dataframe. Solve the rest of the questions using this dataframe copy.**","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/weather-dataset-rattle-package/weatherAUS.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-18T22:36:15.318981Z","iopub.execute_input":"2021-11-18T22:36:15.319383Z","iopub.status.idle":"2021-11-18T22:36:15.731620Z","shell.execute_reply.started":"2021-11-18T22:36:15.319349Z","shell.execute_reply":"2021-11-18T22:36:15.730510Z"},"trusted":true},"execution_count":166,"outputs":[]},{"cell_type":"code","source":"df.describe().T","metadata":{"execution":{"iopub.status.busy":"2021-11-18T22:36:15.733825Z","iopub.execute_input":"2021-11-18T22:36:15.734285Z","iopub.status.idle":"2021-11-18T22:36:15.885356Z","shell.execute_reply.started":"2021-11-18T22:36:15.734228Z","shell.execute_reply":"2021-11-18T22:36:15.884422Z"},"trusted":true},"execution_count":167,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-11-18T22:36:15.886745Z","iopub.execute_input":"2021-11-18T22:36:15.887077Z","iopub.status.idle":"2021-11-18T22:36:16.016722Z","shell.execute_reply.started":"2021-11-18T22:36:15.887034Z","shell.execute_reply":"2021-11-18T22:36:16.015666Z"},"trusted":true},"execution_count":168,"outputs":[]},{"cell_type":"markdown","source":"# **3. Find out the shape and size info of the dataset.**","metadata":{}},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-18T22:36:16.019472Z","iopub.execute_input":"2021-11-18T22:36:16.019816Z","iopub.status.idle":"2021-11-18T22:36:16.025248Z","shell.execute_reply.started":"2021-11-18T22:36:16.019770Z","shell.execute_reply":"2021-11-18T22:36:16.024436Z"},"trusted":true},"execution_count":169,"outputs":[]},{"cell_type":"code","source":"df.size","metadata":{"execution":{"iopub.status.busy":"2021-11-18T22:36:16.026705Z","iopub.execute_input":"2021-11-18T22:36:16.027190Z","iopub.status.idle":"2021-11-18T22:36:16.039317Z","shell.execute_reply.started":"2021-11-18T22:36:16.027148Z","shell.execute_reply":"2021-11-18T22:36:16.038298Z"},"trusted":true},"execution_count":170,"outputs":[]},{"cell_type":"markdown","source":"# **4.Find out the types values of the columns and save the result as a dataframe.**","metadata":{}},{"cell_type":"code","source":"df4 = pd.DataFrame(df.dtypes)\ndf4.columns =[\"dtypes\"]\n\ndf4","metadata":{"execution":{"iopub.status.busy":"2021-11-18T22:36:16.040905Z","iopub.execute_input":"2021-11-18T22:36:16.041419Z","iopub.status.idle":"2021-11-18T22:36:16.057629Z","shell.execute_reply.started":"2021-11-18T22:36:16.041275Z","shell.execute_reply":"2021-11-18T22:36:16.056883Z"},"trusted":true},"execution_count":171,"outputs":[]},{"cell_type":"markdown","source":"# **5.Find out the non-null counts of the columns and save the result as a dataframe.**","metadata":{}},{"cell_type":"code","source":"df5 = pd.DataFrame(df.count())\ndf5.columns = [\"notnull\"]\ndf5","metadata":{"execution":{"iopub.status.busy":"2021-11-18T22:36:16.059247Z","iopub.execute_input":"2021-11-18T22:36:16.059793Z","iopub.status.idle":"2021-11-18T22:36:16.186408Z","shell.execute_reply.started":"2021-11-18T22:36:16.059760Z","shell.execute_reply":"2021-11-18T22:36:16.185550Z"},"trusted":true},"execution_count":172,"outputs":[]},{"cell_type":"markdown","source":"# **6.Find out the null counts of the columns and save the result as a dataframe.**","metadata":{}},{"cell_type":"code","source":"df6 = pd.DataFrame(df.isna().sum())\ndf6.columns = [\"isnull\"]\ndf6","metadata":{"execution":{"iopub.status.busy":"2021-11-18T22:36:16.187674Z","iopub.execute_input":"2021-11-18T22:36:16.187911Z","iopub.status.idle":"2021-11-18T22:36:16.313544Z","shell.execute_reply.started":"2021-11-18T22:36:16.187881Z","shell.execute_reply":"2021-11-18T22:36:16.312504Z"},"trusted":true},"execution_count":173,"outputs":[]},{"cell_type":"markdown","source":"# **7.Find out the unique counts of the columns and save the result as a dataframe.**","metadata":{}},{"cell_type":"code","source":"df7 = pd.DataFrame(df.nunique())\ndf7.columns = [\"nunique\"]\ndf7","metadata":{"execution":{"iopub.status.busy":"2021-11-18T22:36:16.315233Z","iopub.execute_input":"2021-11-18T22:36:16.315546Z","iopub.status.idle":"2021-11-18T22:36:16.454066Z","shell.execute_reply.started":"2021-11-18T22:36:16.315503Z","shell.execute_reply":"2021-11-18T22:36:16.453007Z"},"trusted":true},"execution_count":174,"outputs":[]},{"cell_type":"markdown","source":"# **8.Merge the dataframes you created in questions 4-5-6-7.**","metadata":{}},{"cell_type":"code","source":"frames = [df4,df5,df6,df7]\nresult = pd.concat(frames,axis=1)\nresult.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-18T22:36:16.457143Z","iopub.execute_input":"2021-11-18T22:36:16.457380Z","iopub.status.idle":"2021-11-18T22:36:16.470004Z","shell.execute_reply.started":"2021-11-18T22:36:16.457352Z","shell.execute_reply":"2021-11-18T22:36:16.468906Z"},"trusted":true},"execution_count":175,"outputs":[]},{"cell_type":"markdown","source":"# **9.Lowercase all column names.**","metadata":{}},{"cell_type":"code","source":"df.columns= df.columns.str.lower()\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-18T22:36:16.471754Z","iopub.execute_input":"2021-11-18T22:36:16.472128Z","iopub.status.idle":"2021-11-18T22:36:16.509772Z","shell.execute_reply.started":"2021-11-18T22:36:16.472082Z","shell.execute_reply":"2021-11-18T22:36:16.508863Z"},"trusted":true},"execution_count":176,"outputs":[]},{"cell_type":"markdown","source":"# **10.Change all the No values to NoRain and all the Yes values to Rain in raintoday and raintomorrow columns.**","metadata":{}},{"cell_type":"code","source":"df[\"raintoday\"].replace(\"No\", \"NoRain\")\ndf[\"raintoday\"].replace(\"Yes\", \"Rain\")\ndf[\"raintomorrow\"].replace(\"No\", \"NoRain\")\ndf[\"raintomorrow\"].replace(\"Yes\", \"Rain\")\ndf[[\"raintoday\",\"raintomorrow\"]]","metadata":{"execution":{"iopub.status.busy":"2021-11-18T22:36:16.511165Z","iopub.execute_input":"2021-11-18T22:36:16.511408Z","iopub.status.idle":"2021-11-18T22:36:16.565594Z","shell.execute_reply.started":"2021-11-18T22:36:16.511378Z","shell.execute_reply":"2021-11-18T22:36:16.564759Z"},"trusted":true},"execution_count":177,"outputs":[]},{"cell_type":"markdown","source":"# **11.Change the data type of \"date\" (object) column to datetime64 and reformat the date as DD/MM/YYYY.**","metadata":{}},{"cell_type":"code","source":"pd.to_datetime(df['date'])\n","metadata":{"execution":{"iopub.status.busy":"2021-11-18T22:36:16.568833Z","iopub.execute_input":"2021-11-18T22:36:16.569734Z","iopub.status.idle":"2021-11-18T22:36:16.621367Z","shell.execute_reply.started":"2021-11-18T22:36:16.569694Z","shell.execute_reply":"2021-11-18T22:36:16.620424Z"},"trusted":true},"execution_count":178,"outputs":[]},{"cell_type":"code","source":"df['date'] = pd.to_datetime(df['date'], errors='coerce')\ndf['date'] = df['date'].dt.strftime('%d/%m/%Y')\ndf['date']","metadata":{"execution":{"iopub.status.busy":"2021-11-18T22:36:16.623768Z","iopub.execute_input":"2021-11-18T22:36:16.624123Z","iopub.status.idle":"2021-11-18T22:36:17.758871Z","shell.execute_reply.started":"2021-11-18T22:36:16.624076Z","shell.execute_reply":"2021-11-18T22:36:17.758007Z"},"trusted":true},"execution_count":179,"outputs":[]},{"cell_type":"markdown","source":"# **12.Create a new column called \"difference\", calculate the difference between maxtemp and mintemp columns for each row, and store the value in this new column.**","metadata":{}},{"cell_type":"code","source":"df['difference'] = df['maxtemp'] - df['mintemp']\ndf","metadata":{"execution":{"iopub.status.busy":"2021-11-18T22:36:17.760098Z","iopub.execute_input":"2021-11-18T22:36:17.760331Z","iopub.status.idle":"2021-11-18T22:36:17.865544Z","shell.execute_reply.started":"2021-11-18T22:36:17.760301Z","shell.execute_reply":"2021-11-18T22:36:17.864650Z"},"trusted":true},"execution_count":180,"outputs":[]},{"cell_type":"markdown","source":"# **13.Remove the evaporation and sunshine columns from the dataset permanently.**","metadata":{}},{"cell_type":"code","source":"l = ['evaporation','sunshine']\n#df.drop(l,axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T22:36:17.866946Z","iopub.execute_input":"2021-11-18T22:36:17.867374Z","iopub.status.idle":"2021-11-18T22:36:17.872214Z","shell.execute_reply.started":"2021-11-18T22:36:17.867334Z","shell.execute_reply":"2021-11-18T22:36:17.871381Z"},"trusted":true},"execution_count":181,"outputs":[]},{"cell_type":"markdown","source":"# **14.Find out the most rainy day for each city.**","metadata":{}},{"cell_type":"code","source":"#df.groupby('Location').aggregate({'Rainfall':'max'})\n","metadata":{"execution":{"iopub.status.busy":"2021-11-18T22:36:17.873624Z","iopub.execute_input":"2021-11-18T22:36:17.873891Z","iopub.status.idle":"2021-11-18T22:36:17.887746Z","shell.execute_reply.started":"2021-11-18T22:36:17.873855Z","shell.execute_reply":"2021-11-18T22:36:17.887022Z"},"trusted":true},"execution_count":182,"outputs":[]},{"cell_type":"markdown","source":"# **15.Filter out all the data for the city 'Albury' and then sort according to maxtemp column.**","metadata":{}},{"cell_type":"code","source":"new_df = df[df.location == 'Albury']\nnew_df.sort_values(by=['maxtemp'])","metadata":{"execution":{"iopub.status.busy":"2021-11-18T22:36:17.892038Z","iopub.execute_input":"2021-11-18T22:36:17.892571Z","iopub.status.idle":"2021-11-18T22:36:17.997465Z","shell.execute_reply.started":"2021-11-18T22:36:17.892523Z","shell.execute_reply":"2021-11-18T22:36:17.996363Z"},"trusted":true},"execution_count":183,"outputs":[]},{"cell_type":"markdown","source":"# **16.Find out the NaN counts for each column.**","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-11-18T22:36:17.998855Z","iopub.execute_input":"2021-11-18T22:36:17.999346Z","iopub.status.idle":"2021-11-18T22:36:18.124600Z","shell.execute_reply.started":"2021-11-18T22:36:17.999298Z","shell.execute_reply":"2021-11-18T22:36:18.123614Z"},"trusted":true},"execution_count":184,"outputs":[]},{"cell_type":"markdown","source":"# **17.Remove the rows with NaN values in \"windgustdir\" column from the dataframe permanently.**","metadata":{}},{"cell_type":"code","source":"df.dropna(subset=['windgustdir'],inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T22:36:18.128003Z","iopub.execute_input":"2021-11-18T22:36:18.128233Z","iopub.status.idle":"2021-11-18T22:36:18.169746Z","shell.execute_reply.started":"2021-11-18T22:36:18.128206Z","shell.execute_reply":"2021-11-18T22:36:18.168734Z"},"trusted":true},"execution_count":185,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-11-18T22:36:18.171227Z","iopub.execute_input":"2021-11-18T22:36:18.171554Z","iopub.status.idle":"2021-11-18T22:36:18.288691Z","shell.execute_reply.started":"2021-11-18T22:36:18.171519Z","shell.execute_reply":"2021-11-18T22:36:18.288004Z"},"trusted":true},"execution_count":186,"outputs":[]},{"cell_type":"markdown","source":"# **18.Create a new dataframe, use \"Location\" column as the index of the dataframe, display the min, max, and median values of \"evaporation\" and \"sunshine\" columns in this dataframe.**","metadata":{}},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2021-11-18T22:36:18.290080Z","iopub.execute_input":"2021-11-18T22:36:18.290327Z","iopub.status.idle":"2021-11-18T22:36:18.385716Z","shell.execute_reply.started":"2021-11-18T22:36:18.290299Z","shell.execute_reply":"2021-11-18T22:36:18.384726Z"},"trusted":true},"execution_count":187,"outputs":[]},{"cell_type":"code","source":"df.groupby('location').aggregate({'evaporation':['min','max',np.median],'sunshine':['min','max',np.median]})","metadata":{"execution":{"iopub.status.busy":"2021-11-18T22:36:18.387304Z","iopub.execute_input":"2021-11-18T22:36:18.387707Z","iopub.status.idle":"2021-11-18T22:36:18.451845Z","shell.execute_reply.started":"2021-11-18T22:36:18.387659Z","shell.execute_reply":"2021-11-18T22:36:18.451266Z"},"trusted":true},"execution_count":188,"outputs":[]},{"cell_type":"markdown","source":"# **19.Find out the hottest day of \"Perth\". Example output: Timestamp('2015-01-05 00:00:00')**","metadata":{}},{"cell_type":"code","source":"mask = df['location']=='Perth'\nresult = df[mask].sort_values('maxtemp',ascending=False).head(1)['date']\nresult","metadata":{"execution":{"iopub.status.busy":"2021-11-18T22:36:18.452872Z","iopub.execute_input":"2021-11-18T22:36:18.453207Z","iopub.status.idle":"2021-11-18T22:36:18.482862Z","shell.execute_reply.started":"2021-11-18T22:36:18.453178Z","shell.execute_reply":"2021-11-18T22:36:18.482255Z"},"trusted":true},"execution_count":189,"outputs":[]},{"cell_type":"markdown","source":"# **20.Group your dataframe by location and find out the averages of all numeric values.**","metadata":{}},{"cell_type":"code","source":"df.groupby('location').mean()","metadata":{"execution":{"iopub.status.busy":"2021-11-18T22:36:18.484092Z","iopub.execute_input":"2021-11-18T22:36:18.484466Z","iopub.status.idle":"2021-11-18T22:36:18.565502Z","shell.execute_reply.started":"2021-11-18T22:36:18.484435Z","shell.execute_reply":"2021-11-18T22:36:18.564685Z"},"trusted":true},"execution_count":190,"outputs":[]}]}